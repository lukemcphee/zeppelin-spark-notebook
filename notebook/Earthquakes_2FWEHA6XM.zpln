{
  "paragraphs": [
    {
      "title": "First some do some clean up steps and load seismology data into hadoop.",
      "text": "%sh\ncd /tmp\nif [ -e /tmp/GEM-GHEC-v1.txt ]\nthen\n    rm -f /tmp/GEM-GHEC-v1.txt\nfi\nif [ -e /tmp/isc-gem-cat.zip ]\nthen\n    rm -f /tmp/isc-gem-cat.zip\nfi\nif [ -e /tmp/isc-gem-cat.csv ]\nthen\n    rm -f /tmp/isc-gem-cat.csv\nfi \n\nif hadoop fs -stat /tmp/isc-gem-cat.csv\nthen\n   hadoop fs -rm  /tmp/isc-gem-cat.csv\nfi\nif hadoop fs -stat /tmp/GEM-GHEC-v1.txt\nthen\n   hadoop fs -rm  /tmp/GEM-GHEC-v1.txt\nfi\n\nwget https://dl.dropboxusercontent.com/u/114020/zeppelin-snapshots/datasets/earthquake/GEM-GHEC-v1.txt\nwget https://dl.dropboxusercontent.com/u/114020/zeppelin-snapshots/datasets/earthquake/isc-gem-cat.zip\nunzip isc-gem-cat.zip\nhadoop fs -put GEM-GHEC-v1.txt /tmp\nhadoop fs -put isc-gem-cat.csv /tmp\necho \"done\"",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:21.274",
      "config": {
        "lineNumbers": false,
        "tableHide": false,
        "editorMode": "ace/mode/sh",
        "colWidth": 12.0,
        "editorHide": false,
        "title": true,
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "sh",
          "editOnDblClick": false,
          "completionSupport": false
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "bash: line 14: hadoop: command not found\nbash: line 18: hadoop: command not found\n--2021-02-07 16:01:27--  https://dl.dropboxusercontent.com/u/114020/zeppelin-snapshots/datasets/earthquake/GEM-GHEC-v1.txt\nResolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.64.15, 2620:100:6020:15::a27d:400f\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.64.15|:443... connected.\nHTTP request sent, awaiting response... 404 Not Found\n2021-02-07 16:01:28 ERROR 404: Not Found.\n\n--2021-02-07 16:01:28--  https://dl.dropboxusercontent.com/u/114020/zeppelin-snapshots/datasets/earthquake/isc-gem-cat.zip\nResolving dl.dropboxusercontent.com (dl.dropboxusercontent.com)... 162.125.64.15, 2620:100:6020:15::a27d:400f\nConnecting to dl.dropboxusercontent.com (dl.dropboxusercontent.com)|162.125.64.15|:443... connected.\nHTTP request sent, awaiting response... 404 Not Found\n2021-02-07 16:01:28 ERROR 404: Not Found.\n\nunzip:  cannot find or open isc-gem-cat.zip, isc-gem-cat.zip.zip or isc-gem-cat.zip.ZIP.\nbash: line 26: hadoop: command not found\nbash: line 27: hadoop: command not found\ndone\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672322_477086662",
      "id": "20150506-001716_1056821854",
      "dateCreated": "2021-02-07 16:01:12.323",
      "dateStarted": "2021-02-07 16:01:21.399",
      "dateFinished": "2021-02-07 16:01:28.606",
      "status": "FINISHED"
    },
    {
      "text": "%md\n###Make a  Spark RDD and a schema to go with it. Perform some more clean up steps of the data. ",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:28.638",
      "config": {
        "tableHide": false,
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0,
        "editorHide": true,
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003e###Make a  Spark RDD and a schema to go with it. Perform some more clean up steps of the data.\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672347_1684898731",
      "id": "20150727-062937_662448974",
      "dateCreated": "2021-02-07 16:01:12.347",
      "dateStarted": "2021-02-07 16:01:28.744",
      "dateFinished": "2021-02-07 16:01:28.831",
      "status": "FINISHED"
    },
    {
      "text": "//create RDD\nval historicalData \u003d sc.textFile(\"/tmp/GEM-GHEC-v1.txt\")\n\n//define the schema\nval globalInstrumentalCatalogData \u003d sc.textFile(\"/tmp/isc-gem-cat.csv\")\ncase class EarthQuake(\nid: String,\ndate: String,\nlat: Double,\nlon: Double,\ndepth: Double,\nmag: Double,\nmunc: Double\n)\n\n//clean up the records. Delimit by tabs and peel out the year/month\n//remove unwanted data entries\nval historical \u003d historicalData.filter(s \u003d\u003e\n!s.startsWith(\"#\") \u0026\u0026 !s.startsWith(\"En\\tSource\")\n).map{s\u003d\u003e\n//make a function to peel out the year\ndef get(s:String) \u003d {\nif (s\u003d\u003d\"\") {\n\"00\"\n} else {\ns\n}\n}\nval r \u003d s.split(\"\\t\")\nval year \u003d r(2) // year\nif (r(2) !\u003d \"\") {\nval month \u003d r(2)\n} else {\n//...and peel out the month\nval month \u003d \"00\"\n}\n//format the date\nval date \u003d r(2) + \"-\" + get(r(3)) + \"-\" + get(r(4)) + \" \" + get(r(5)) + \":\" + get(r(6)) + \":\" + get(r(7)) + \".00\"\n\n//extract interesting earthquake data:  id, date, depth, magnitude, \"uncorrected\" magnitude\nEarthQuake(\nr(0).toString, // id\ndate,\nget(r(9)).toDouble,\nget(r(10)).toDouble,\nget(r(14)).toDouble, // depth\nget(r(17)).toDouble, // mag\nget(r(18)).toDouble // mag unc\n\n)\n}\n\nval earthQuake \u003d globalInstrumentalCatalogData.filter(!_.startsWith(\"#\")).map{s\u003d\u003e\nval r \u003d s.split(\",\")\nEarthQuake(\nr(23).trim,\nr(0).trim,\nr(1).toDouble,\nr(2).toDouble,\nr(7).toDouble,\nr(10).toDouble,\nr(11).toDouble\n)\n}.union(historical).toDF   //make a dataframe\n//make a table to query\nearthQuake.registerTempTable(\"eq\")",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:28.836",
      "config": {
        "tableHide": false,
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0,
        "editorHide": false,
        "enabled": true,
        "graph": {
          "mode": "stackedAreaChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "dt",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "avg",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "dt",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "avg",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[33mwarning: \u001b[0mthere was one deprecation warning; re-run with -deprecation for details\norg.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/tmp/isc-gem-cat.csv\n  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\n  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\n  at org.apache.spark.SparkContext$$anonfun$union$1$$anonfun$31.apply(SparkContext.scala:1314)\n  at org.apache.spark.SparkContext$$anonfun$union$1$$anonfun$31.apply(SparkContext.scala:1314)\n  at scala.collection.TraversableLike$$anonfun$filterImpl$1.apply(TraversableLike.scala:248)\n  at scala.collection.immutable.List.foreach(List.scala:392)\n  at scala.collection.TraversableLike$class.filterImpl(TraversableLike.scala:247)\n  at scala.collection.TraversableLike$class.filter(TraversableLike.scala:259)\n  at scala.collection.AbstractTraversable.filter(Traversable.scala:104)\n  at org.apache.spark.SparkContext$$anonfun$union$1.apply(SparkContext.scala:1314)\n  at org.apache.spark.SparkContext$$anonfun$union$1.apply(SparkContext.scala:1313)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.SparkContext.withScope(SparkContext.scala:699)\n  at org.apache.spark.SparkContext.union(SparkContext.scala:1313)\n  at org.apache.spark.SparkContext$$anonfun$union$2.apply(SparkContext.scala:1325)\n  at org.apache.spark.SparkContext$$anonfun$union$2.apply(SparkContext.scala:1325)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.SparkContext.withScope(SparkContext.scala:699)\n  at org.apache.spark.SparkContext.union(SparkContext.scala:1324)\n  at org.apache.spark.rdd.RDD$$anonfun$union$1.apply(RDD.scala:625)\n  at org.apache.spark.rdd.RDD$$anonfun$union$1.apply(RDD.scala:625)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n  at org.apache.spark.rdd.RDD.union(RDD.scala:624)\n  ... 39 elided\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672347_297351096",
      "id": "20150506-195215_1965567578",
      "dateCreated": "2021-02-07 16:01:12.347",
      "dateStarted": "2021-02-07 16:01:28.986",
      "dateFinished": "2021-02-07 16:01:32.965",
      "status": "ERROR"
    },
    {
      "text": "",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:33.035",
      "config": {
        "tableHide": false,
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0,
        "editorHide": false,
        "enabled": false,
        "graph": {
          "mode": "stackedAreaChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "dt",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "cnt",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "dt",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "cnt",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672348_1601960219",
      "id": "20150507-003446_1538551325",
      "dateCreated": "2021-02-07 16:01:12.348",
      "status": "FINISHED"
    },
    {
      "title": "",
      "text": "%md\n### Display the average magnitude, maximum magnitude, and minimum magnitude for all the earthquakes per year.",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:33.253",
      "config": {
        "tableHide": false,
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0,
        "editorHide": true,
        "title": false,
        "enabled": true,
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "dt",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "depth",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "dt",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "depth",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eDisplay the average magnitude, maximum magnitude, and minimum magnitude for all the earthquakes per year.\u003c/h3\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672349_1827650267",
      "id": "20150507-003532_1237133223",
      "dateCreated": "2021-02-07 16:01:12.349",
      "dateStarted": "2021-02-07 16:01:33.304",
      "dateFinished": "2021-02-07 16:01:33.360",
      "status": "FINISHED"
    },
    {
      "text": "%sql select substring(date, 0, 4) as dt, avg(mag) as avg, max(mag) as max, min(mag) as min from eq group by substring(date, 0, 4) order by dt",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:33.408",
      "config": {
        "tableHide": false,
        "editorMode": "ace/mode/sql",
        "colWidth": 12.0,
        "editorHide": false,
        "enabled": true,
        "graph": {
          "mode": "lineChart",
          "height": 300.0,
          "optionOpen": true,
          "keys": [
            {
              "name": "dt",
              "index": 0.0,
              "aggr": "sum",
              "$$hashKey": "object:5432"
            }
          ],
          "values": [
            {
              "name": "avg",
              "index": 1.0,
              "aggr": "sum",
              "$$hashKey": "object:5441"
            },
            {
              "name": "max",
              "index": 2.0,
              "aggr": "sum",
              "$$hashKey": "object:5442"
            },
            {
              "name": "min",
              "index": 3.0,
              "aggr": "sum",
              "$$hashKey": "object:5443"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "dt",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "avg",
              "index": 1.0,
              "aggr": "sum"
            }
          },
          "lineWithFocus": false,
          "forceY": false
        },
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Error happens in sql: select substring(date, 0, 4) as dt, avg(mag) as avg, max(mag) as max, min(mag) as min from eq group by substring(date, 0, 4) order by dt\njava.lang.reflect.InvocationTargetException\n\tat sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.zeppelin.spark.SparkSqlInterpreter.internalInterpret(SparkSqlInterpreter.java:105)\n\tat org.apache.zeppelin.interpreter.AbstractInterpreter.interpret(AbstractInterpreter.java:47)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:110)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:776)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:668)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:172)\n\tat org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:130)\n\tat org.apache.zeppelin.scheduler.ParallelScheduler.lambda$runJobInScheduler$0(ParallelScheduler.java:39)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.AnalysisException: Table or view not found: eq; line 1 pos 91\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:731)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:683)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:713)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:706)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:706)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:652)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)\n\t... 14 more\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view \u0027eq\u0027 not found in database \u0027default\u0027;\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalog$class.requireTableExists(ExternalCatalog.scala:48)\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.requireTableExists(InMemoryCatalog.scala:45)\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.getTable(InMemoryCatalog.scala:326)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:706)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:728)\n\t... 67 more\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672349_1254718609",
      "id": "20150507-003619_868555740",
      "dateCreated": "2021-02-07 16:01:12.349",
      "dateStarted": "2021-02-07 16:01:33.486",
      "dateFinished": "2021-02-07 16:01:33.601",
      "status": "ERROR"
    },
    {
      "text": "%md\n## Make another table with the average depth for all earthquakes that year.",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:33.708",
      "config": {
        "tableHide": false,
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0,
        "editorHide": true,
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eMake another table with the average depth for all earthquakes that year.\u003c/h2\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672349_1410447218",
      "id": "20150507-003703_1041146387",
      "dateCreated": "2021-02-07 16:01:12.350",
      "dateStarted": "2021-02-07 16:01:33.756",
      "dateFinished": "2021-02-07 16:01:33.776",
      "status": "FINISHED"
    },
    {
      "title": "Average depth  (Click on different displays)",
      "text": "%sql select substring(date, 0, 4) as dt, avg(depth) as depth from eq group by substring(date, 0, 4) order by dt",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:33.855",
      "config": {
        "tableHide": false,
        "editorMode": "ace/mode/sql",
        "colWidth": 12.0,
        "editorHide": false,
        "title": true,
        "enabled": true,
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "dt",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "depth",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "dt",
              "index": 0.0,
              "aggr": "sum"
            }
          }
        },
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Error happens in sql: select substring(date, 0, 4) as dt, avg(depth) as depth from eq group by substring(date, 0, 4) order by dt\njava.lang.reflect.InvocationTargetException\n\tat sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.zeppelin.spark.SparkSqlInterpreter.internalInterpret(SparkSqlInterpreter.java:105)\n\tat org.apache.zeppelin.interpreter.AbstractInterpreter.interpret(AbstractInterpreter.java:47)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:110)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:776)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:668)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:172)\n\tat org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:130)\n\tat org.apache.zeppelin.scheduler.ParallelScheduler.lambda$runJobInScheduler$0(ParallelScheduler.java:39)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.AnalysisException: Table or view not found: eq; line 1 pos 61\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:731)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:683)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:713)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:706)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:706)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:652)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)\n\t... 14 more\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view \u0027eq\u0027 not found in database \u0027default\u0027;\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalog$class.requireTableExists(ExternalCatalog.scala:48)\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.requireTableExists(InMemoryCatalog.scala:45)\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.getTable(InMemoryCatalog.scala:326)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:706)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:728)\n\t... 67 more\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672351_323614716",
      "id": "20150507-003750_2126368247",
      "dateCreated": "2021-02-07 16:01:12.351",
      "dateStarted": "2021-02-07 16:01:33.926",
      "dateFinished": "2021-02-07 16:01:33.972",
      "status": "ERROR"
    },
    {
      "text": "%md\n## Plot the number of recorded earthquakes per year",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:34.023",
      "config": {
        "tableHide": false,
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0,
        "editorHide": true,
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003ePlot the number of recorded earthquakes per year\u003c/h2\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672351_893271893",
      "id": "20150507-003813_123197788",
      "dateCreated": "2021-02-07 16:01:12.351",
      "dateStarted": "2021-02-07 16:01:34.057",
      "dateFinished": "2021-02-07 16:01:34.093",
      "status": "FINISHED"
    },
    {
      "title": "Counts over time.   Change the year in the box",
      "text": "%sql \nselect substring(date, 0, 4) as dt, count(*) as cnt \nfrom eq \nwhere substring(date, 0, 4) \u003e ${dt\u003d1800}\ngroup by substring(date, 0, 4) \norder by dt",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:34.153",
      "config": {
        "tableHide": false,
        "editorMode": "ace/mode/sql",
        "colWidth": 12.0,
        "editorHide": false,
        "title": true,
        "enabled": true,
        "graph": {
          "mode": "multiBarChart",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "dt",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "cnt",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "dt",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "cnt",
              "index": 1.0,
              "aggr": "sum"
            }
          },
          "lineWithFocus": true
        },
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "sql",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {
          "{dt": "1",
          "dt": "1900",
          "dt \u003e 1900": ""
        },
        "forms": {
          "dt": {
            "type": "TextBox",
            "name": "dt",
            "displayName": "dt",
            "defaultValue": "1800",
            "hidden": false
          }
        }
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "Error happens in sql: select substring(date, 0, 4) as dt, count(*) as cnt \nfrom eq \nwhere substring(date, 0, 4) \u003e 1900\ngroup by substring(date, 0, 4) \norder by dt\njava.lang.reflect.InvocationTargetException\n\tat sun.reflect.GeneratedMethodAccessor66.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.zeppelin.spark.SparkSqlInterpreter.internalInterpret(SparkSqlInterpreter.java:105)\n\tat org.apache.zeppelin.interpreter.AbstractInterpreter.interpret(AbstractInterpreter.java:47)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:110)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:776)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:668)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:172)\n\tat org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:130)\n\tat org.apache.zeppelin.scheduler.ParallelScheduler.lambda$runJobInScheduler$0(ParallelScheduler.java:39)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.AnalysisException: Table or view not found: eq; line 2 pos 5\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:731)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:683)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:713)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:706)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:706)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:652)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n\tat org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)\n\t... 14 more\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view \u0027eq\u0027 not found in database \u0027default\u0027;\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalog$class.requireTableExists(ExternalCatalog.scala:48)\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.requireTableExists(InMemoryCatalog.scala:45)\n\tat org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.getTable(InMemoryCatalog.scala:326)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:706)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:728)\n\t... 77 more\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672352_214114786",
      "id": "20160914-162058_982126158",
      "dateCreated": "2021-02-07 16:01:12.353",
      "dateStarted": "2021-02-07 16:01:34.217",
      "dateFinished": "2021-02-07 16:01:34.291",
      "status": "ERROR"
    },
    {
      "text": "%md\n## Make a table of the aggregate number of earthquakes of varying magnitude. ",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:34.391",
      "config": {
        "tableHide": false,
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0,
        "editorHide": true,
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "mag",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "count",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "mag",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "count",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eMake a table of the aggregate number of earthquakes of varying magnitude.\u003c/h2\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672354_1379416674",
      "id": "20160914-182236_1084497064",
      "dateCreated": "2021-02-07 16:01:12.354",
      "dateStarted": "2021-02-07 16:01:34.445",
      "dateFinished": "2021-02-07 16:01:34.467",
      "status": "FINISHED"
    },
    {
      "text": "println(\"%table mag\\tcount\")\nearthQuake.map(s\u003d\u003e(s(5).toString.toDouble.toInt, 1)).reduceByKey(_ + _).collect.foreach(s\u003d\u003eprintln(s._1 + \"\\t\" + s._2))",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:34.525",
      "config": {
        "tableHide": false,
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0,
        "editorHide": false,
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "mag",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "count",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "mag",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "count",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003cconsole\u003e:35: \u001b[31merror: \u001b[0mnot found: value earthQuake\n       earthQuake.map(s\u003d\u003e(s(5).toString.toDouble.toInt, 1)).reduceByKey(_ + _).collect.foreach(s\u003d\u003eprintln(s._1 + \"\\t\" + s._2))\n       ^\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672354_326473352",
      "id": "20160914-204946_925542696",
      "dateCreated": "2021-02-07 16:01:12.354",
      "dateStarted": "2021-02-07 16:01:34.580",
      "dateFinished": "2021-02-07 16:01:34.761",
      "status": "ERROR"
    },
    {
      "text": "%md\nHere is some code to prepare a map visualization...",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:34.861",
      "config": {
        "tableHide": false,
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0,
        "editorHide": true,
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003eHere is some code to prepare a map visualization\u0026hellip;\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672354_1725849419",
      "id": "20160914-204959_1347008603",
      "dateCreated": "2021-02-07 16:01:12.354",
      "dateStarted": "2021-02-07 16:01:34.922",
      "dateFinished": "2021-02-07 16:01:34.986",
      "status": "FINISHED"
    },
    {
      "text": "val gson \u003d new com.google.gson.Gson()\nval bubbles \u003d sqlContext.sql(s\"\"\"select *, case when mag \u003c 6 then 1.7 else ((mag - 6) * 3)+1.7 end as mag_adjusted,  case when depth \u003c\u003d 100 then cast(depth/10 as int) when depth \u003c\u003d 1000 then cast(depth/100 as int) *10  else 100 end  as depth_cat from eq where date \u003e\u003d \"1990\" \"\"\").map{r\u003d\u003e\nMap(\n\"name\" -\u003e r(0).toString,\n\"radius\" -\u003e r(7).toString.toDouble,\n\"mag\" -\u003e r(5).toString.toDouble,\n\"date\" -\u003e r(1).toString,\n\"latitude\" -\u003e r(2).toString.toDouble,\n\"longitude\" -\u003e r(3).toString.toDouble,\n\"fillKey\" -\u003e r(8).toString,\n\"depth\" -\u003e r(4).toString\n)\n}.collect.map(s\u003d\u003escala.collection.JavaConversions.mapAsJavaMap(s))\nval bubblesJson \u003d gson.toJson(bubbles)",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:35.022",
      "config": {
        "tableHide": true,
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0,
        "editorHide": false,
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.spark.sql.AnalysisException: Table or view not found: eq; line 1 pos 218\n  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:731)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:683)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:713)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:706)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:328)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:326)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:706)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:652)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n  at scala.collection.immutable.List.foldLeft(List.scala:84)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n  at scala.collection.immutable.List.foreach(List.scala:392)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)\n  at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)\n  ... 47 elided\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view \u0027eq\u0027 not found in database \u0027default\u0027;\n  at org.apache.spark.sql.catalyst.catalog.ExternalCatalog$class.requireTableExists(ExternalCatalog.scala:48)\n  at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.requireTableExists(InMemoryCatalog.scala:45)\n  at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.getTable(InMemoryCatalog.scala:326)\n  at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)\n  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:706)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:728)\n  ... 100 more\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672354_885249453",
      "id": "20160914-205026_1677625084",
      "dateCreated": "2021-02-07 16:01:12.354",
      "dateStarted": "2021-02-07 16:01:35.116",
      "dateFinished": "2021-02-07 16:01:38.343",
      "status": "ERROR"
    },
    {
      "text": "%md \n...and more scala code to support displaying html and a world map.",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:38.367",
      "config": {
        "tableHide": false,
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0,
        "editorHide": true,
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003e\u0026hellip;and more scala code to support displaying html and a world map.\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672354_1708998075",
      "id": "20160914-210314_1506482357",
      "dateCreated": "2021-02-07 16:01:12.354",
      "dateStarted": "2021-02-07 16:01:38.525",
      "dateFinished": "2021-02-07 16:01:38.570",
      "status": "FINISHED"
    },
    {
      "title": "World map showing the location of earthquakes between the years 1000-2009.  (Hover mouse to see more detail)",
      "text": "println(s\"\"\"%html\n\u003c!-- script src\u003d\"//cdnjs.cloudflare.com/ajax/libs/d3/3.5.3/d3.min.js\"\u003e\u003c/script --\u003e\n\u003cscript src\u003d\"http://datamaps.github.io/scripts/topojson.js\"\u003e\u003c/script\u003e\n\u003cscript src\u003d\"http://datamaps.github.io/scripts/0.4.0/datamaps.all.js\"\u003e\u003c/script\u003e\n\u003cdiv id\u003d\"worldMap\" style\u003d\"position: relative; width: 1400px; height: 1200px;\"\u003e\u003c/div\u003e\n\u003cscript\u003e\nsetTimeout(function(){\nvar map \u003d new Datamap({\nelement: document.getElementById(\u0027worldMap\u0027),\ngeographyConfig: {\npopupOnHover: true,\nhighlightOnHover: false\n},\nfills: {\ndefaultFill: \u0027#ABDDA4\u0027,\n\u00270\u0027: \u0027#393b79\u0027,\n\u00271\u0027: \u0027#5254a3\u0027,\n\u00272\u0027: \u0027#6b6ecf\u0027,\n\u00273\u0027: \u0027#9c9ede\u0027,\n\u00274\u0027: \u0027#637939\u0027,\n\u00275\u0027: \u0027#8ca252\u0027,\n\u00276\u0027: \u0027#b5cf6b\u0027,\n\u00277\u0027: \u0027#cedb9c\u0027,\n\u00278\u0027: \u0027#8c6d31\u0027,\n\u00279\u0027: \u0027#bd9e39\u0027,\n\u002710\u0027: \u0027#e7ba52\u0027,\n\u002720\u0027: \u0027#e7cb94\u0027,\n\u002730\u0027: \u0027#843c39\u0027,\n\u002740\u0027: \u0027#ad494a\u0027,\n\u002750\u0027: \u0027#d6616b\u0027,\n\u002760\u0027: \u0027#e7969c\u0027,\n\u002770\u0027: \u0027#7b4173\u0027,\n\u002780\u0027: \u0027#a55194\u0027,\n\u002790\u0027: \u0027#ce6dbd\u0027,\n\u0027100\u0027: \u0027#de9ed6\u0027\n},\nbubblesConfig: {\nborderWidth: 0,\nfillOpacity: 0.75\n}\n});\n\nmap.bubbles(${bubblesJson}, {\npopupTemplate: function(geo, data) {\nreturn \u0027\u003cdiv class\u003d\"hoverinfo\"\u003eMagnitude: \u0027 + data.mag + \u0027\u003cbr/\u003e  Exploded on: \u0027 + data.date + \u0027 \u003cbr/\u003e Depth: \u0027 + data.depth + \u0027\u003cbr/\u003e Depth Category: \u0027 + data.fillKey +  \u0027\u003c/div\u003e\u0027\n}\n});\n\n}, 2000)\n\u003c/script\u003e\n\"\"\")",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:38.608",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003cconsole\u003e:75: \u001b[31merror: \u001b[0mnot found: value bubblesJson\nmap.bubbles(${bubblesJson}, {\n              ^\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672354_545818489",
      "id": "20160914-211029_145460768",
      "dateCreated": "2021-02-07 16:01:12.354",
      "dateStarted": "2021-02-07 16:01:38.695",
      "dateFinished": "2021-02-07 16:01:38.936",
      "status": "ERROR"
    },
    {
      "text": "%md \n## Now lets run a machine learning algorithm to predict ... the probabilty of a magnitude 6.0 or higher.\n### We\u0027ll logistic regression classification",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:38.980",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eNow lets run a machine learning algorithm to predict \u0026hellip; the probabilty of a magnitude 6.0 or higher.\u003c/h2\u003e\n\u003ch3\u003eWe\u0026rsquo;ll logistic regression classification\u003c/h3\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672354_1906963230",
      "id": "20160914-213524_799610539",
      "dateCreated": "2021-02-07 16:01:12.354",
      "dateStarted": "2021-02-07 16:01:39.049",
      "dateFinished": "2021-02-07 16:01:39.096",
      "status": "FINISHED"
    },
    {
      "text": "",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:39.140",
      "config": {
        "tableHide": true,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": false,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672359_319107688",
      "id": "20160914-233239_1995820851",
      "dateCreated": "2021-02-07 16:01:12.360",
      "status": "FINISHED"
    },
    {
      "text": "%md\n## First import various Spark libaries ",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:39.224",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eFirst import various Spark libaries\u003c/h2\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672360_117808822",
      "id": "20160915-162231_1581637092",
      "dateCreated": "2021-02-07 16:01:12.360",
      "dateStarted": "2021-02-07 16:01:39.266",
      "dateFinished": "2021-02-07 16:01:39.301",
      "status": "FINISHED"
    },
    {
      "text": "import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.ml.feature.Bucketizer\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature.PCA\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.feature.Binarizer",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:39.359",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.ml.feature.Bucketizer\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.feature.PCA\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.feature.Binarizer\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672360_2096627285",
      "id": "20160916-002547_103900922",
      "dateCreated": "2021-02-07 16:01:12.360",
      "dateStarted": "2021-02-07 16:01:39.423",
      "dateFinished": "2021-02-07 16:01:40.259",
      "status": "FINISHED"
    },
    {
      "title": "",
      "text": "%md\n## Let\u0027s look at a row of the earthquake data and create a dataframe (with year instead of full date).",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:40.310",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "title": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eLet\u0026rsquo;s look at a row of the earthquake data and create a dataframe (with year instead of full date).\u003c/h2\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672360_2096514841",
      "id": "20160916-002854_1799999999",
      "dateCreated": "2021-02-07 16:01:12.360",
      "dateStarted": "2021-02-07 16:01:40.445",
      "dateFinished": "2021-02-07 16:01:40.489",
      "status": "FINISHED"
    },
    {
      "text": "earthQuake.show(1)\nval eq2 \u003d earthQuake.selectExpr(\"id\", \"substr(date,1,4) as year\", \"lat\", \"lon\", \"depth\", \"mag\", \"munc\").toDF",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:40.545",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003cconsole\u003e:39: \u001b[31merror: \u001b[0mnot found: value earthQuake\n       val eq2 \u003d earthQuake.selectExpr(\"id\", \"substr(date,1,4) as year\", \"lat\", \"lon\", \"depth\", \"mag\", \"munc\").toDF\n                 ^\n\u003cconsole\u003e:38: \u001b[31merror: \u001b[0mnot found: value earthQuake\n       earthQuake.show(1)\n       ^\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672360_22916714",
      "id": "20160916-003607_1809817186",
      "dateCreated": "2021-02-07 16:01:12.360",
      "dateStarted": "2021-02-07 16:01:40.825",
      "dateFinished": "2021-02-07 16:01:40.970",
      "status": "ERROR"
    },
    {
      "title": "Spark likes numeric types. Cast all columns to double",
      "text": "val df \u003d eq2.select(eq2(\"id\").cast(\"double\"),\neq2(\"year\").cast(\"double\"),\neq2(\"lat\").cast(\"double\"),\neq2(\"lon\").cast(\"double\"),\neq2(\"depth\").cast(\"double\"),\neq2(\"mag\").cast(\"double\"),\neq2(\"munc\").cast(\"double\"))\n\ndf.show(3)",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:41.015",
      "config": {
        "tableHide": true,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003cconsole\u003e:40: \u001b[31merror: \u001b[0mnot found: value eq2\n       val df \u003d eq2.select(eq2(\"id\").cast(\"double\"),\n                ^\n\u003cconsole\u003e:40: \u001b[31merror: \u001b[0mnot found: value eq2\n       val df \u003d eq2.select(eq2(\"id\").cast(\"double\"),\n                           ^\n\u003cconsole\u003e:41: \u001b[31merror: \u001b[0mnot found: value eq2\n       eq2(\"year\").cast(\"double\"),\n       ^\n\u003cconsole\u003e:42: \u001b[31merror: \u001b[0mnot found: value eq2\n       eq2(\"lat\").cast(\"double\"),\n       ^\n\u003cconsole\u003e:43: \u001b[31merror: \u001b[0mnot found: value eq2\n       eq2(\"lon\").cast(\"double\"),\n       ^\n\u003cconsole\u003e:44: \u001b[31merror: \u001b[0mnot found: value eq2\n       eq2(\"depth\").cast(\"double\"),\n       ^\n\u003cconsole\u003e:45: \u001b[31merror: \u001b[0mnot found: value eq2\n       eq2(\"mag\").cast(\"double\"),\n       ^\n\u003cconsole\u003e:46: \u001b[31merror: \u001b[0mnot found: value eq2\n       eq2(\"munc\").cast(\"double\"))\n       ^\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672360_16793042",
      "id": "20160916-003700_1717253157",
      "dateCreated": "2021-02-07 16:01:12.360",
      "dateStarted": "2021-02-07 16:01:41.194",
      "dateFinished": "2021-02-07 16:01:41.358",
      "status": "ERROR"
    },
    {
      "title": "Now create training and testing data sets.  (60% for training and 40% for testing)",
      "text": "val Array(training, testing) \u003d df.randomSplit(Array(0.6, 0.4))",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:41.397",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mtraining\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m \u003d [location: string, iso_code: string ... 10 more fields]\n\u001b[1m\u001b[34mtesting\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m \u003d [location: string, iso_code: string ... 10 more fields]\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672360_824211783",
      "id": "20160916-003759_2016839031",
      "dateCreated": "2021-02-07 16:01:12.360",
      "dateStarted": "2021-02-07 16:01:41.499",
      "dateFinished": "2021-02-07 16:01:43.556",
      "status": "FINISHED"
    },
    {
      "title": "We\u0027ll use a VectorAssembler to combine many features into one feature column (logistic regression likes that)",
      "text": "import org.apache.spark.ml.feature.VectorAssembler\nval assembler \u003d new VectorAssembler()\n  .setInputCols(Array(\"id\", \"year\", \"lat\", \"lon\", \"depth\", \"mag\", \"munc\"))\n  .setOutputCol(\"featureSet\")",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:43.586",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.spark.ml.feature.VectorAssembler\n\u001b[1m\u001b[34massembler\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.ml.feature.VectorAssembler\u001b[0m \u003d vecAssembler_391d80f04d6b\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672360_562248412",
      "id": "20160916-014948_2144160198",
      "dateCreated": "2021-02-07 16:01:12.360",
      "dateStarted": "2021-02-07 16:01:43.751",
      "dateFinished": "2021-02-07 16:01:44.562",
      "status": "FINISHED"
    },
    {
      "title": " Set the binary threshold to Richter scale Magnitude of 6.0",
      "text": "val binaryClassifier \u003d new Binarizer().setInputCol(\"mag\").setOutputCol(\"binaryLabel\").setThreshold(6.0)",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:44.664",
      "config": {
        "lineNumbers": true,
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mbinaryClassifier\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.ml.feature.Binarizer\u001b[0m \u003d binarizer_6600eaed35ce\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672360_1059892970",
      "id": "20160916-015226_1618097247",
      "dateCreated": "2021-02-07 16:01:12.361",
      "dateStarted": "2021-02-07 16:01:44.848",
      "dateFinished": "2021-02-07 16:01:45.552",
      "status": "FINISHED"
    },
    {
      "title": "Create a logistic regression model and set some parameters",
      "text": "val lr \u003d new LogisticRegression().setMaxIter(20).setRegParam(0.2).setElasticNetParam(0.8).setLabelCol(\"binaryLabel\").setFeaturesCol(\"featureSet\")",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:45.647",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mlr\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.ml.classification.LogisticRegression\u001b[0m \u003d logreg_54113a7a7a32\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672361_1246315799",
      "id": "20160916-015507_1061729146",
      "dateCreated": "2021-02-07 16:01:12.361",
      "dateStarted": "2021-02-07 16:01:45.807",
      "dateFinished": "2021-02-07 16:01:46.519",
      "status": "FINISHED"
    },
    {
      "title": "Chain everything together with a Spark Pipeline (great way to organize steps)",
      "text": "val pipeline \u003d new Pipeline()\n  .setStages(Array(assembler, binaryClassifier, lr))",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:46.600",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mpipeline\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.ml.Pipeline\u001b[0m \u003d pipeline_43c07eb45764\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672361_1482376873",
      "id": "20160916-020040_433824923",
      "dateCreated": "2021-02-07 16:01:12.361",
      "dateStarted": "2021-02-07 16:01:46.826",
      "dateFinished": "2021-02-07 16:01:47.806",
      "status": "FINISHED"
    },
    {
      "title": "Train the model",
      "text": "\nval model \u003d pipeline.fit(training)",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:47.928",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "java.lang.IllegalArgumentException: Field \"id\" does not exist.\nAvailable fields: location, iso_code, date, total_vaccinations, people_vaccinated, people_fully_vaccinated, daily_vaccinations_raw, daily_vaccinations, total_vaccinations_per_hundred, people_vaccinated_per_hundred, people_fully_vaccinated_per_hundred, daily_vaccinations_per_million\n  at org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n  at org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:274)\n  at scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\n  at scala.collection.AbstractMap.getOrElse(Map.scala:59)\n  at org.apache.spark.sql.types.StructType.apply(StructType.scala:273)\n  at org.apache.spark.ml.feature.VectorAssembler$$anonfun$6.apply(VectorAssembler.scala:162)\n  at org.apache.spark.ml.feature.VectorAssembler$$anonfun$6.apply(VectorAssembler.scala:161)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n  at scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:186)\n  at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:161)\n  at org.apache.spark.ml.Pipeline$$anonfun$transformSchema$4.apply(Pipeline.scala:184)\n  at org.apache.spark.ml.Pipeline$$anonfun$transformSchema$4.apply(Pipeline.scala:184)\n  at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\n  at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\n  at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:186)\n  at org.apache.spark.ml.Pipeline.transformSchema(Pipeline.scala:184)\n  at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n  at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:136)\n  ... 47 elided\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672361_1906430105",
      "id": "20160916-030853_1507812713",
      "dateCreated": "2021-02-07 16:01:12.361",
      "dateStarted": "2021-02-07 16:01:48.025",
      "dateFinished": "2021-02-07 16:01:49.165",
      "status": "ERROR"
    },
    {
      "title": "Make predictions",
      "text": "val prediction \u003d model.transform(testing)",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:49.258",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003cconsole\u003e:41: \u001b[31merror: \u001b[0mnot found: value model\n       val prediction \u003d model.transform(testing)\n                        ^\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672361_1263929138",
      "id": "20160916-030907_2071741220",
      "dateCreated": "2021-02-07 16:01:12.361",
      "dateStarted": "2021-02-07 16:01:49.535",
      "dateFinished": "2021-02-07 16:01:49.820",
      "status": "ERROR"
    },
    {
      "title": "Display the results",
      "text": "prediction.select(\"prediction\", \"binaryLabel\", \"mag\").show(10)",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:49.841",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003cconsole\u003e:40: \u001b[31merror: \u001b[0mnot found: value prediction\n       prediction.select(\"prediction\", \"binaryLabel\", \"mag\").show(10)\n       ^\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672361_1880419543",
      "id": "20160916-031504_513861686",
      "dateCreated": "2021-02-07 16:01:12.374",
      "dateStarted": "2021-02-07 16:01:50.026",
      "dateFinished": "2021-02-07 16:01:50.209",
      "status": "ERROR"
    },
    {
      "text": "%md\n##How well did we do? We need to quantify the error.",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:50.222",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003e##How well did we do? We need to quantify the error.\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672375_1171373683",
      "id": "20160916-045911_240632767",
      "dateCreated": "2021-02-07 16:01:12.375",
      "dateStarted": "2021-02-07 16:01:50.284",
      "dateFinished": "2021-02-07 16:01:50.356",
      "status": "FINISHED"
    },
    {
      "text": "%md\n## Let\u0027s create a dataframe that just identifies the correct and incorrect predictions.    \n###(1 is correct and 0 is incorrect)\n",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:50.379",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/markdown",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch2\u003eLet\u0026rsquo;s create a dataframe that just identifies the correct and incorrect predictions.\u003c/h2\u003e\n\u003cp\u003e###(1 is correct and 0 is incorrect)\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672375_1611661950",
      "id": "20160916-045924_596326585",
      "dateCreated": "2021-02-07 16:01:12.375",
      "dateStarted": "2021-02-07 16:01:50.616",
      "dateFinished": "2021-02-07 16:01:50.686",
      "status": "FINISHED"
    },
    {
      "text": "\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:50.710",
      "config": {
        "tableHide": true,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672375_1897676589",
      "id": "20160917-193257_979788123",
      "dateCreated": "2021-02-07 16:01:12.375",
      "status": "FINISHED"
    },
    {
      "text": "val matches \u003d udf((A : Int, B: Int) \u003d\u003e {\n    if (A+B \u003d\u003d 1) 0\n    else 1\n})\n\nval total \u003d prediction.count\nval rightWrong \u003d prediction.withColumn(\"matches\", matches($\"prediction\", $\"binaryLabel\")).groupBy(\"matches\").count.toDF\nrightWrong.registerTempTable(\"rightWrong\")\nrightWrong.show",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:50.909",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003cconsole\u003e:44: \u001b[31merror: \u001b[0mnot found: value prediction\n       val total \u003d prediction.count\n                   ^\n\u003cconsole\u003e:45: \u001b[31merror: \u001b[0mnot found: value prediction\n       val rightWrong \u003d prediction.withColumn(\"matches\", matches($\"prediction\", $\"binaryLabel\")).groupBy(\"matches\").count.toDF\n                        ^\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672375_192055777",
      "id": "20160917-203745_1685937666",
      "dateCreated": "2021-02-07 16:01:12.375",
      "dateStarted": "2021-02-07 16:01:50.988",
      "dateFinished": "2021-02-07 16:01:51.459",
      "status": "ERROR"
    },
    {
      "title": "Accuracy \u003d  Correct / Total",
      "text": "rightWrong.where($\"matches\"\u003d\u003d\u003d1).select(rightWrong(\"count\") / total* 100 ).show",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:51.471",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "title": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u003cconsole\u003e:40: \u001b[31merror: \u001b[0mnot found: value rightWrong\n       rightWrong.where($\"matches\"\u003d\u003d\u003d1).select(rightWrong(\"count\") / total* 100 ).show\n       ^\n\u003cconsole\u003e:40: \u001b[31merror: \u001b[0mnot found: value rightWrong\n       rightWrong.where($\"matches\"\u003d\u003d\u003d1).select(rightWrong(\"count\") / total* 100 ).show\n                                               ^\n\u003cconsole\u003e:40: \u001b[31merror: \u001b[0mnot found: value total\n       rightWrong.where($\"matches\"\u003d\u003d\u003d1).select(rightWrong(\"count\") / total* 100 ).show\n                                                                     ^\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672375_2065094837",
      "id": "20160918-212718_729378682",
      "dateCreated": "2021-02-07 16:01:12.375",
      "dateStarted": "2021-02-07 16:01:51.582",
      "dateFinished": "2021-02-07 16:01:51.817",
      "status": "ERROR"
    },
    {
      "text": "%md\n####Not too bad an accuracy. ",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:51.895",
      "config": {
        "tableHide": false,
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "editorHide": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {}
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003cp\u003e####Not too bad an accuracy.\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672378_1868148045",
      "id": "20160918-213443_542169457",
      "dateCreated": "2021-02-07 16:01:12.378",
      "dateStarted": "2021-02-07 16:01:51.963",
      "dateFinished": "2021-02-07 16:01:51.985",
      "status": "FINISHED"
    },
    {
      "text": "",
      "user": "anonymous",
      "dateUpdated": "2021-02-07 16:01:52.057",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": false,
        "fontSize": 9.0,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1612713672379_1961047606",
      "id": "20160919-070638_1285110803",
      "dateCreated": "2021-02-07 16:01:12.379",
      "status": "FINISHED"
    }
  ],
  "name": "Earthquakes",
  "id": "2FWEHA6XM",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0-preview2",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {
    "isRunning": true
  }
}